#!/usr/bin/env python
# coding: utf-8

import torch, accelerate, time
import chevron
import logging
from functools import reduce
from pathlib import Path
from dataclasses import dataclass
from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, \
                         StoppingCriteria, StoppingCriteriaList, \
                         pipeline, GenerationConfig

from model_api_server.datatype import ChatRecord, Role

@dataclass
class ChatTuple:
    """
    Grouped chat record for rendering prompt.
    """
    system:str = None
    user:str = None
    bot:str = None

class ChatTupleFactory:
    
    @staticmethod
    def from_chat_history(chat_history: [ChatRecord]) -> [ChatTuple]:
        """
        Convert a list of ChatRecords to a list o ChatTuple.
        Noticed that it's expected that a bot message is an immediate successor of a user message.
        """
        chat_tuples = []
        for chat in chat_history:
            if chat.role == Role.USER:
                chat_tuples.append(ChatTuple(user=chat.msg))
            elif chat.role == Role.BOT:
                chat_tuples[-1].bot = chat.msg
        return chat_tuples

class StopOnTokens(StoppingCriteria):
    def __init__(self, tokenizer: AutoTokenizer):
        stop_list = ['<s>', '</s>', '[INST]', '\nQuestion:', "[INST: ]"]
        to_token_id = lambda x: torch.LongTensor(tokenizer(x)['input_ids']).to('cuda')
        self.stop_token_ids = map(to_token_id, stop_list)
    
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for stop_ids in self.stop_token_ids:
            if torch.all(input_ids[0][-len(stop_ids):] == stop_ids):
                return True
        return False

class TaideLlm:
    def __init__(self,
                 token_limit = 3500,
                 model_path = '/llm/llama2-7b-chat-b1.0.0',
                 prompt_template_path = 'prompt_template/taide.mustache'):
        self.logger = logging.getLogger(__name__)
        
        self.input_token_limit = token_limit

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            config=AutoConfig.from_pretrained(model_path),
            device_map="auto",
            torch_dtype=torch.float16
        )
        model.eval()

        self.tokenizer = AutoTokenizer.from_pretrained(model_path)

        self.pipe = pipeline(
            model=model,
            tokenizer=self.tokenizer,
            return_full_text=False,
            task='text-generation',
            stopping_criteria=StoppingCriteriaList([StopOnTokens(self.tokenizer)]),
            temperature=0.2,
            max_new_tokens=2048,
            repetition_penalty = 1.0,
            do_sample=True
        )

        prompt_template_file = Path(prompt_template_path)
        self.prompt_template = prompt_template_file.read_text()

    def is_too_long(self, chat_history: [ChatRecord]) -> bool:
        """
        Estimate whether the prompt generated by the given chat history will be too long.
        This public API can be use to evaluate how many documents can be placed in the context window.
        """
        
        chat_history = ChatTupleFactory.from_chat_history(chat_history)
        prompt = self.gen_prompt(chat_history)
        return self._is_too_long(prompt)
    
    def _is_too_long(self, sentence: str) -> bool:
        """
        Calculate whether the number of tokens of given sentence exceeds the threshold.
        """

        num_tokens = len(self.tokenizer.tokenize(sentence))
        return num_tokens >= self.input_token_limit

    def gen_prompt(self, chat_history: [ChatTuple], append_system: bool = True) -> str:
        """
        Generate prompt from given chat history.
        """

        system_chat_tuple = ChatTuple(
            system = 'You are a helpful assistant. 你是一個樂於助人的助手。',
            user = '請用中文回答我',
            bot = '好! 我樂於助人,是你的好助手。'
        )

        if append_system:
            chat_history = [system_chat_tuple] + chat_history

        prompt = chevron.render(
            self.prompt_template,
            {'history': chat_history}
        )

        return prompt
    

    async def complete(self, chat_history: [ChatRecord]): 
        result = ''
        try:
            
            chat_history = ChatTupleFactory.from_chat_history(chat_history)
            self.logger.info('Data: {}'.format(chat_history))
            
            # Trim the over-length history
            prompt = ''
            while True:
                prompt = self.gen_prompt(chat_history)
                if not self._is_too_long(prompt): break
                chat_history = chat_history[1:]
            
            self.logger.info('Final Prompt: {}'.format(prompt))
            
            self.logger.info('Generating...')
            result = self.pipe(prompt)[0]['generated_text']
            self.logger.info('Generation finished.')
            
        except Exception as e:
            result = ''
            self.logger.exception('Generation failed.')
        finally:
            # torch.cuda.empty_cache()
            return result
